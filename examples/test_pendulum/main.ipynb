{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from lagom import BaseAlgorithm\n",
    "from lagom.agents import A2CAgent\n",
    "from lagom.core.policies import CategoricalMLPPolicy\n",
    "from lagom.core.utils import Logger\n",
    "from lagom.runner import Runner\n",
    "from lagom.envs import EnvSpec\n",
    "\n",
    "from lagom.utils import set_global_seeds\n",
    "\n",
    "from engine import GoalEngine\n",
    "\n",
    "from goal_sampler import LinearGoalSampler\n",
    "from goal_sampler import SWUCBgGoalSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\t Reward : 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuo/anaconda3/envs/RL_server/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\t Reward : 9.0\n",
      "Episode 30\t Reward : 12.0\n",
      "Episode 40\t Reward : 8.0\n",
      "Episode 50\t Reward : 10.0\n",
      "Episode 60\t Reward : 8.0\n",
      "Episode 70\t Reward : 8.0\n",
      "Episode 80\t Reward : 9.0\n",
      "Episode 90\t Reward : 12.0\n",
      "Episode 100\t Reward : 10.0\n",
      "Episode 110\t Reward : 10.0\n",
      "Episode 120\t Reward : 10.0\n",
      "Episode 130\t Reward : 10.0\n",
      "Episode 140\t Reward : 10.0\n",
      "Episode 150\t Reward : 8.0\n",
      "Episode 160\t Reward : 10.0\n",
      "Episode 170\t Reward : 10.0\n",
      "Episode 180\t Reward : 9.0\n",
      "Episode 190\t Reward : 8.0\n",
      "Episode 200\t Reward : 10.0\n",
      "Episode 210\t Reward : 10.0\n",
      "Episode 220\t Reward : 9.0\n",
      "Episode 230\t Reward : 10.0\n",
      "Episode 240\t Reward : 11.0\n",
      "Episode 250\t Reward : 19.0\n",
      "Episode 260\t Reward : 10.0\n",
      "Episode 270\t Reward : 118.0\n",
      "Episode 280\t Reward : 129.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "args = args()\n",
    "args.gamma = 0.99\n",
    "args.seed = 1\n",
    "args.render = False\n",
    "args.log_interval = 10\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-2, alpha=0.99, eps=1e-5)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(Variable(state))\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.data[0]\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + float(np.finfo(np.float32).eps))\n",
    "    for (log_prob, value), r in zip(saved_actions, rewards):\n",
    "        reward = r - value.data[0]\n",
    "        policy_losses.append(-log_prob * reward)\n",
    "        value_losses.append(F.smooth_l1_loss(value, Variable(torch.Tensor([r]))))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state = env.reset()\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            \n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            model.rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards = np.sum(model.rewards)\n",
    "        \n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print(f'Episode {i_episode}\\t Reward : {rewards}')\n",
    "            \n",
    "        if rewards == 200:\n",
    "            print('Solved!')\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
